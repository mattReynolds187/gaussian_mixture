"""
A fully vectorized implementation of a Gaussian Mixture Model
for matrix completion using the Expectation Maximization Algorithm.
"""

import numpy as np
import matplotlib.pyplot as plt

class GMM:
    def __init__(self, X, K, seed):
        """
        args:
            X - (nxd np.ndarray)
                the matrix to be completed. Assumes 0 denotes missing entry.
                n represents the number of data points
                d represents the dimension of the data
            K - (int)
                number of clusters to be generated by the model
            seed - (int) for random initialization of mean vectors
        """
        np.random.seed(seed)
        self.X = X
        self.n = X.shape[0]
        self.d = X.shape[1]
        self.k = K

        #delta is an nxd matrix of zeroes and ones representing where X has missing data
        #(the uth row of delta corresponds to C_u from the notebook)
        #(also corresponds to delta(l|C_u) from notebook)
        self.delta = np.where(self.X == 0, 0, 1)

        #random initialization of Kxd mean matrix. Each row represents the mean of
        #a gaussian component (corresponds to mu from the notebook)
        self.mu = X[np.random.choice(X.shape[0], K, replace=False)]

        #initialization of the weights for each cluster
        #(corresponds to pi from the notebook)
        self.p = np.ones(K)/K

        #initialization of shape Kx1 variance vector
        #each component represents the variance of a gaussian cluster
        #(corresponds to sigma^2 from the notebook)
        self.var = np.sum((self.mu*np.ones([self.n, self.k, self.d]) - X.reshape([self.n, 1, self.d]))**2, 
                           axis=(0,2))/(self.n*self.d)
        
        #a list to store the log_likelihood at each step of the EM algorithm
        self.history = []

        #initializaing ||x-mu||^2 across all data as seen in the Gaussian density function 
        #will be used in log likelihood function and m-step
        self.norm_squared = self.compute_norm_squared()

    def compute_norm_squared(self):
        """
        Vectorized computation of ||X-mu||^2 across all data.
        Used in both log likelihood function and m-step.
        """
        delta_reshaped = self.delta.reshape([self.n, 1, self.d]) #nx1xd
        X_reshaped = self.X.reshape([self.n, 1, self.d])
        u_3d = (self.mu*np.ones([self.n, self.k, self.d]))*delta_reshaped
        sub_stack = u_3d-X_reshaped #nxkxd

        return np.sum(sub_stack**2, axis = 2)#nxk


    def logged_gauss(self):
        """
        Computes and returns the log of the Gaussian density function across all data
        (corresponds to N() in the log likelihood function)
        """
        exp_factor_logged = -self.norm_squared/(2*self.var)

        C_u = np.sum(self.delta, axis = 1, keepdims=True)
        var_2d = self.var*np.ones([self.n, self.k])
        first_factor_logged = np.log(self.p) - (C_u/2)*np.log(2*np.pi*var_2d)

        return first_factor_logged + exp_factor_logged

    def estep(self):
        """
        Performs the expectation step of the EM algorithm. Uses LogSumExp trick
        to help ensure numerical stability.
        
        Returns:
            Post - nxK np.ndarray (corresponds to P(j|u) from the notebook)
            Log Likelihood - current value of the log likelihood function
        """
        logged_gauss = self.logged_gauss()
        max_vector = np.amax(logged_gauss, axis=1, keepdims=True)
        scaled_gauss = np.exp(logged_gauss - max_vector)
        denominator_logged = max_vector + np.log(np.sum(scaled_gauss, axis = 1, keepdims=True))
        log_post = logged_gauss - denominator_logged
        log_likelihood = np.sum(denominator_logged)

        return np.exp(log_post), log_likelihood

    def mstep(self, post, min_variance):
        """
        Performs the maximization step of the EM algorithm and updates the parameters. Since 
        no regularization is implemented we use a minimum variance to control for vanishing variance.
        
        args:
            post - Kxn np.ndarray (corresponds to P(j|u) from the notebook)
            min_variance - lower bound on the variance allowed
        """
        #update mu
        mu_numerator = np.dot(self.X.T, post).T
        mu_denominator = np.dot(self.delta.T, post).T
        self.mu = np.where(mu_denominator >= 1, mu_numerator/(mu_denominator + 1e-16), self.mu) #kxd

        #update variance
        C_u = np.sum(self.delta, axis = 1, keepdims=True)
        summation_factor = np.sum(post*self.norm_squared, axis = 0)
        first_factor = 1/np.sum(C_u*post, axis = 0)
        var_bad = first_factor*summation_factor
        self.var = np.where(var_bad < min_variance, min_variance, var_bad)

        #updating the weights for each cluster
        self.p = np.sum(post , axis = 0)/self.n

        #updating norm_squared
        self.norm_squared = self.compute_norm_squared()

    def run(self, min_variance=.25):
        flag = False
        while True:
            post, new_log_likelihood = self.estep()
            self.history.append(new_log_likelihood)
            if flag and new_log_likelihood - old_log_likelihood <= abs(new_log_likelihood)/(10**6):
                break
            old_log_likelihood = new_log_likelihood
            self.mstep(post, min_variance)
            flag = True

    def fill_matrix(self, to_fill):
        """
        Fills the matrix using the completed model.
        """
        post = self.estep()[0]
        new_values = np.dot(post, self.mu)
        filled = np.where(to_fill==0, new_values, to_fill)

        return filled

    def plot_history(self):
        plt.plot(self.history)
        plt.ylabel('Log Likelihood')
        plt.xlabel('EM Algorithm Step')
        plt.show()

    @staticmethod
    def evaluate_rmse(filled_matrix, test_matrix):
        return np.sqrt(np.mean((filled_matrix - test_matrix)**2))
